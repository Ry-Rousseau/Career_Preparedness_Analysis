{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0214e7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4211 entries, 0 to 4210\n",
      "Data columns (total 69 columns):\n",
      " #   Column                                Non-Null Count  Dtype   \n",
      "---  ------                                --------------  -----   \n",
      " 0   country                               4211 non-null   object  \n",
      " 1   sector                                4211 non-null   object  \n",
      " 2   country_sector                        4211 non-null   category\n",
      " 3   gender                                4211 non-null   category\n",
      " 4   age                                   4211 non-null   category\n",
      " 5   pers_extraverted_enthusiastic         4211 non-null   Int64   \n",
      " 6   pers_critical_quarrelsome             4211 non-null   Int64   \n",
      " 7   pers_dependable_self-disciplined      4211 non-null   Int64   \n",
      " 8   pers_anxious_easily_upset             4211 non-null   Int64   \n",
      " 9   pers_open_to_new_experiences_complex  4211 non-null   Int64   \n",
      " 10  pers_sympathetic_warm                 4211 non-null   Int64   \n",
      " 11  pers_disorganized_careless            4211 non-null   Int64   \n",
      " 12  pers_reserved_quiet                   4211 non-null   Int64   \n",
      " 13  pers_calm_emotionally_stable          4211 non-null   Int64   \n",
      " 14  pers_conventional_uncreative          4211 non-null   Int64   \n",
      " 15  prep_level                            4211 non-null   float32 \n",
      " 16  qual_reasons                          4189 non-null   object  \n",
      " 17  el_lbal_anx                           4211 non-null   Int64   \n",
      " 18  el_lbal_sat                           4211 non-null   Int64   \n",
      " 19  el_lcnt_com                           4211 non-null   Int64   \n",
      " 20  el_lcnt_psy                           4211 non-null   Int64   \n",
      " 21  el_lcon_inc                           4211 non-null   Int64   \n",
      " 22  el_lcon_soc                           4211 non-null   Int64   \n",
      " 23  el_lcont_con                          4211 non-null   Int64   \n",
      " 24  el_lcont_imp                          4211 non-null   Int64   \n",
      " 25  el_lgen                               4211 non-null   Int64   \n",
      " 26  el_ljou_opt                           4211 non-null   Int64   \n",
      " 27  el_ljou_pro                           4211 non-null   Int64   \n",
      " 28  el_lres_fin                           4211 non-null   Int64   \n",
      " 29  el_lres_ski                           4211 non-null   Int64   \n",
      " 30  el_ltra_aut                           4211 non-null   Int64   \n",
      " 31  el_ltra_des                           4211 non-null   Int64   \n",
      " 32  wfl_lcarprom_d                        4211 non-null   Int64   \n",
      " 33  wfl_lcollea                           4211 non-null   Int64   \n",
      " 34  wfl_lenjhyb                           4211 non-null   Int64   \n",
      " 35  wfl_lexcess_wk                        4210 non-null   Int64   \n",
      " 36  wfl_lfin_lair                         4210 non-null   Int64   \n",
      " 37  wfl_llearn_dev                        4211 non-null   Int64   \n",
      " 38  wfl_lmean_full                        4210 non-null   Int64   \n",
      " 39  wfl_lpoorman                          4211 non-null   Int64   \n",
      " 40  wfl_ltoxic                            4211 non-null   Int64   \n",
      " 41  wfl_luse_skills                       4210 non-null   Int64   \n",
      " 42  wfl_lwellcomp                         4211 non-null   Int64   \n",
      " 43  es_fbal_anx                           4210 non-null   float32 \n",
      " 44  es_fbal_sat                           4211 non-null   float32 \n",
      " 45  es_fcnt_com                           4211 non-null   float32 \n",
      " 46  es_fcnt_psy                           4211 non-null   float32 \n",
      " 47  es_fcon_inc                           4211 non-null   float32 \n",
      " 48  es_fcon_soc                           4209 non-null   float32 \n",
      " 49  es_fcont_con                          4210 non-null   float32 \n",
      " 50  es_fcont_imp                          4210 non-null   float32 \n",
      " 51  es_fgen                               4210 non-null   float32 \n",
      " 52  es_fjou_opt                           4211 non-null   float32 \n",
      " 53  es_fjou_pro                           4209 non-null   float32 \n",
      " 54  es_fres_fin                           4206 non-null   float32 \n",
      " 55  es_fres_ski                           4211 non-null   float32 \n",
      " 56  es_ftra_aut                           4211 non-null   float32 \n",
      " 57  es_ftra_des                           4210 non-null   float32 \n",
      " 58  wfs_fcarprom                          4210 non-null   float32 \n",
      " 59  wfs_fcollea                           4211 non-null   float32 \n",
      " 60  wfs_fenjhyb                           4211 non-null   float32 \n",
      " 61  wfs_fexcess_wk                        4211 non-null   float32 \n",
      " 62  wfs_ffin_fair                         4211 non-null   float32 \n",
      " 63  wfs_flearn_dev                        4211 non-null   float32 \n",
      " 64  wfs_fmean_full                        4211 non-null   float32 \n",
      " 65  wfs_fpoorman                          4211 non-null   float32 \n",
      " 66  wfs_ftoxic                            4208 non-null   float32 \n",
      " 67  wfs_fuse_skills                       4209 non-null   float32 \n",
      " 68  wfs_fwellcomp                         4211 non-null   float32 \n",
      "dtypes: Int64(36), category(3), float32(27), object(3)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_pickle(\"../data/survey_data_cleaned.pkl\")\n",
    "\n",
    "data.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb92a59",
   "metadata": {},
   "source": [
    "#### Part 1: Global Drivers of Preparedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f781bd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LIKERT MODEL SUMMARY ===\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:             prep_level   R-squared:                       0.741\n",
      "Model:                            OLS   Adj. R-squared:                  0.739\n",
      "Method:                 Least Squares   F-statistic:                     351.5\n",
      "Date:                Tue, 05 Aug 2025   Prob (F-statistic):               0.00\n",
      "Time:                        10:02:41   Log-Likelihood:                -4972.2\n",
      "No. Observations:                4210   AIC:                         1.001e+04\n",
      "Df Residuals:                    4175   BIC:                         1.024e+04\n",
      "Df Model:                          34                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===============================================================================================\n",
      "                                  coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Intercept                       1.4476      0.124     11.638      0.000       1.204       1.691\n",
      "age[T.25-40]                    0.0957      0.030      3.144      0.002       0.036       0.155\n",
      "age[T.41-64]                    0.0548      0.032      1.724      0.085      -0.008       0.117\n",
      "country_sector[T.FR Tech]       0.2397      0.049      4.928      0.000       0.144       0.335\n",
      "country_sector[T.IT Fin]        0.3831      0.047      8.145      0.000       0.291       0.475\n",
      "country_sector[T.SP Fin]        0.3673      0.048      7.657      0.000       0.273       0.461\n",
      "country_sector[T.UK Energy]     0.2544      0.046      5.482      0.000       0.163       0.345\n",
      "country_sector[T.US Pharma]     0.3321      0.048      6.944      0.000       0.238       0.426\n",
      "country_sector[T.US Tech]       0.2865      0.047      6.126      0.000       0.195       0.378\n",
      "el_lbal_anx                    -0.0101      0.006     -1.688      0.091      -0.022       0.002\n",
      "el_lbal_sat                    -0.0150      0.008     -1.810      0.070      -0.031       0.001\n",
      "el_lcnt_com                    -0.0022      0.008     -0.270      0.787      -0.018       0.014\n",
      "el_lcnt_psy                    -0.0048      0.009     -0.549      0.583      -0.022       0.012\n",
      "el_lcon_inc                     0.0161      0.008      1.979      0.048       0.000       0.032\n",
      "el_lcon_soc                     0.0073      0.006      1.191      0.234      -0.005       0.019\n",
      "el_lcont_con                    0.0104      0.009      1.177      0.239      -0.007       0.028\n",
      "el_lcont_imp                    0.0116      0.009      1.295      0.195      -0.006       0.029\n",
      "el_lgen                         0.5648      0.009     63.601      0.000       0.547       0.582\n",
      "el_ljou_opt                     0.0059      0.007      0.806      0.420      -0.008       0.020\n",
      "el_ljou_pro                     0.0217      0.008      2.813      0.005       0.007       0.037\n",
      "el_lres_fin                     0.0074      0.006      1.153      0.249      -0.005       0.020\n",
      "el_lres_ski                     0.0523      0.009      5.520      0.000       0.034       0.071\n",
      "el_ltra_aut                     0.0159      0.009      1.768      0.077      -0.002       0.033\n",
      "el_ltra_des                     0.0369      0.007      5.643      0.000       0.024       0.050\n",
      "wfl_lcarprom_d                  0.0150      0.014      1.046      0.296      -0.013       0.043\n",
      "wfl_lcollea                    -0.0044      0.013     -0.347      0.729      -0.029       0.021\n",
      "wfl_lenjhyb                     0.0452      0.011      3.959      0.000       0.023       0.068\n",
      "wfl_lexcess_wk                  0.0107      0.010      1.054      0.292      -0.009       0.030\n",
      "wfl_lfin_lair                   0.0386      0.011      3.565      0.000       0.017       0.060\n",
      "wfl_llearn_dev                 -0.0021      0.015     -0.138      0.890      -0.031       0.027\n",
      "wfl_lmean_full                  0.0274      0.015      1.886      0.059      -0.001       0.056\n",
      "wfl_lpoorman                    0.0171      0.011      1.486      0.137      -0.005       0.040\n",
      "wfl_ltoxic                      0.0076      0.011      0.688      0.491      -0.014       0.029\n",
      "wfl_luse_skills                 0.0157      0.016      1.005      0.315      -0.015       0.046\n",
      "wfl_lwellcomp                   0.0200      0.012      1.620      0.105      -0.004       0.044\n",
      "==============================================================================\n",
      "Omnibus:                      370.387   Durbin-Watson:                   1.937\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              567.457\n",
      "Skew:                          -0.672   Prob(JB):                    6.00e-124\n",
      "Kurtosis:                       4.195   Cond. No.                         287.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "=== FAST CHOICE MODEL SUMMARY ===\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:             prep_level   R-squared:                       0.525\n",
      "Model:                            OLS   Adj. R-squared:                  0.521\n",
      "Method:                 Least Squares   F-statistic:                     135.2\n",
      "Date:                Tue, 05 Aug 2025   Prob (F-statistic):               0.00\n",
      "Time:                        10:02:41   Log-Likelihood:                -6227.6\n",
      "No. Observations:                4194   AIC:                         1.253e+04\n",
      "Df Residuals:                    4159   BIC:                         1.275e+04\n",
      "Df Model:                          34                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===============================================================================================\n",
      "                                  coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Intercept                       5.6047      0.107     52.359      0.000       5.395       5.815\n",
      "age[T.25-40]                    0.1479      0.041      3.589      0.000       0.067       0.229\n",
      "age[T.41-64]                    0.0796      0.043      1.871      0.061      -0.004       0.163\n",
      "country_sector[T.FR Tech]       0.2103      0.064      3.301      0.001       0.085       0.335\n",
      "country_sector[T.IT Fin]        0.1400      0.064      2.173      0.030       0.014       0.266\n",
      "country_sector[T.SP Fin]        0.1505      0.065      2.318      0.020       0.023       0.278\n",
      "country_sector[T.UK Energy]    -0.1211      0.064     -1.903      0.057      -0.246       0.004\n",
      "country_sector[T.US Pharma]     0.2267      0.065      3.491      0.000       0.099       0.354\n",
      "country_sector[T.US Tech]       0.0707      0.064      1.107      0.268      -0.055       0.196\n",
      "es_fbal_anx                    -0.0917      0.047     -1.970      0.049      -0.183      -0.000\n",
      "es_fbal_sat                    -0.0548      0.045     -1.213      0.225      -0.143       0.034\n",
      "es_fcnt_com                    -0.0016      0.047     -0.035      0.972      -0.095       0.091\n",
      "es_fcnt_psy                     0.1077      0.051      2.130      0.033       0.009       0.207\n",
      "es_fcon_inc                    -0.0151      0.045     -0.337      0.736      -0.103       0.073\n",
      "es_fcon_soc                    -0.0360      0.049     -0.730      0.465      -0.133       0.061\n",
      "es_fcont_con                    0.1401      0.051      2.740      0.006       0.040       0.240\n",
      "es_fcont_imp                    0.2360      0.048      4.906      0.000       0.142       0.330\n",
      "es_fgen                         2.1653      0.051     42.333      0.000       2.065       2.266\n",
      "es_fjou_opt                     0.2357      0.046      5.174      0.000       0.146       0.325\n",
      "es_fjou_pro                     0.3295      0.048      6.817      0.000       0.235       0.424\n",
      "es_fres_fin                     0.1197      0.048      2.502      0.012       0.026       0.214\n",
      "es_fres_ski                     0.1355      0.052      2.624      0.009       0.034       0.237\n",
      "es_ftra_aut                     0.1648      0.051      3.238      0.001       0.065       0.265\n",
      "es_ftra_des                     0.4761      0.047     10.162      0.000       0.384       0.568\n",
      "wfs_fcarprom                    0.1631      0.070      2.321      0.020       0.025       0.301\n",
      "wfs_fcollea                     0.0120      0.070      0.171      0.864      -0.126       0.150\n",
      "wfs_fenjhyb                     0.1657      0.069      2.414      0.016       0.031       0.300\n",
      "wfs_fexcess_wk                  0.1253      0.058      2.152      0.031       0.011       0.239\n",
      "wfs_ffin_fair                   0.0186      0.074      0.253      0.800      -0.126       0.163\n",
      "wfs_flearn_dev                 -0.0052      0.073     -0.071      0.943      -0.148       0.137\n",
      "wfs_fmean_full                  0.0914      0.070      1.299      0.194      -0.047       0.229\n",
      "wfs_fpoorman                    0.0401      0.062      0.642      0.521      -0.082       0.163\n",
      "wfs_ftoxic                      0.0627      0.064      0.979      0.328      -0.063       0.188\n",
      "wfs_fuse_skills                 0.0689      0.075      0.917      0.359      -0.078       0.216\n",
      "wfs_fwellcomp                   0.1659      0.064      2.608      0.009       0.041       0.291\n",
      "==============================================================================\n",
      "Omnibus:                      414.183   Durbin-Watson:                   1.977\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              598.926\n",
      "Skew:                          -0.763   Prob(JB):                    8.81e-131\n",
      "Kurtosis:                       4.047   Cond. No.                         21.3\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "=== MODEL COMPARISON ===\n",
      "Likert Model - R²: 0.741, Adj R²: 0.739, N: 4210.0\n",
      "Fast Choice Model - R²: 0.525, Adj R²: 0.521, N: 4194.0\n",
      "\n",
      "=== TOP SIGNIFICANT PREDICTORS (p < 0.05) ===\n",
      "LIKERT MODEL:\n",
      "                             coefficient       p_value\n",
      "el_lgen                         0.564799  0.000000e+00\n",
      "Intercept                       1.447626  7.873642e-31\n",
      "country_sector[T.IT Fin]        0.383051  4.972213e-16\n",
      "country_sector[T.SP Fin]        0.367295  2.342370e-14\n",
      "country_sector[T.US Pharma]     0.332120  4.388705e-12\n",
      "country_sector[T.US Tech]       0.286540  9.857808e-10\n",
      "el_ltra_des                     0.036905  1.781821e-08\n",
      "el_lres_ski                     0.052339  3.592997e-08\n",
      "\n",
      "FAST CHOICE MODEL:\n",
      "                             coefficient       p_value\n",
      "Intercept                       5.604735  0.000000e+00\n",
      "es_fgen                         2.165285  0.000000e+00\n",
      "es_ftra_des                     0.476107  5.546415e-24\n",
      "es_fjou_pro                     0.329482  1.062531e-11\n",
      "es_fjou_opt                     0.235686  2.397994e-07\n",
      "es_fcont_imp                    0.236013  9.645052e-07\n",
      "age[T.25-40]                    0.147866  3.358589e-04\n",
      "country_sector[T.US Pharma]     0.226685  4.866218e-04\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# LIKERT MODEL\n",
    "likert_data = data[['prep_level'] + \n",
    "                   [col for col in data.columns if col.startswith('el_')] +\n",
    "                   [col for col in data.columns if col.startswith('wfl_')] +\n",
    "                   ['age', 'country_sector']].copy()\n",
    "\n",
    "# Transform emotional: -100 to +100 with 9 points → 1 to 9\n",
    "el_cols = [col for col in likert_data.columns if col.startswith('el_')]\n",
    "for col in el_cols:\n",
    "    likert_data[col] = (likert_data[col] + 100) / 25 + 1\n",
    "\n",
    "# Transform workplace: 0 to 100 with 5 points → 1 to 5\n",
    "wfl_cols = [col for col in likert_data.columns if col.startswith('wfl_')]\n",
    "for col in wfl_cols:\n",
    "    likert_data[col] = likert_data[col] / 25 + 1\n",
    "\n",
    "# Drop missing values\n",
    "likert_clean = likert_data.dropna()\n",
    "\n",
    "# Fit model using statsmodels for summary output\n",
    "model_likert = ols('prep_level ~ ' + ' + '.join([col for col in likert_clean.columns if col != 'prep_level']), \n",
    "                   data=likert_clean).fit()\n",
    "\n",
    "# FAST CHOICE MODEL\n",
    "fast_data = data[['prep_level'] + \n",
    "                 [col for col in data.columns if col.startswith('es_')] +\n",
    "                 [col for col in data.columns if col.startswith('wfs_')] +\n",
    "                 ['age', 'country_sector']].copy()\n",
    "\n",
    "# Transform emotional: -100 to +100 → -1 to +1\n",
    "es_cols = [col for col in fast_data.columns if col.startswith('es_')]\n",
    "for col in es_cols:\n",
    "    fast_data[col] = fast_data[col] / 100\n",
    "\n",
    "# Transform workplace: 0 to 100 → 0 to 1\n",
    "wfs_cols = [col for col in fast_data.columns if col.startswith('wfs_')]\n",
    "for col in wfs_cols:\n",
    "    fast_data[col] = fast_data[col] / 100\n",
    "\n",
    "# Drop missing values\n",
    "fast_clean = fast_data.dropna()\n",
    "\n",
    "# Fit model\n",
    "model_fast = ols('prep_level ~ ' + ' + '.join([col for col in fast_clean.columns if col != 'prep_level']), \n",
    "                 data=fast_clean).fit()\n",
    "\n",
    "# RESULTS\n",
    "print(\"=== LIKERT MODEL SUMMARY ===\")\n",
    "print(model_likert.summary())\n",
    "\n",
    "print(\"\\n=== FAST CHOICE MODEL SUMMARY ===\")  \n",
    "print(model_fast.summary())\n",
    "\n",
    "# MODEL COMPARISON TABLE\n",
    "print(\"\\n=== MODEL COMPARISON ===\")\n",
    "print(f\"Likert Model - R²: {model_likert.rsquared:.3f}, Adj R²: {model_likert.rsquared_adj:.3f}, N: {model_likert.nobs}\")\n",
    "print(f\"Fast Choice Model - R²: {model_fast.rsquared:.3f}, Adj R²: {model_fast.rsquared_adj:.3f}, N: {model_fast.nobs}\")\n",
    "\n",
    "# TOP SIGNIFICANT PREDICTORS\n",
    "print(\"\\n=== TOP SIGNIFICANT PREDICTORS (p < 0.05) ===\")\n",
    "print(\"LIKERT MODEL:\")\n",
    "likert_results = pd.DataFrame({\n",
    "    'coefficient': model_likert.params,\n",
    "    'p_value': model_likert.pvalues\n",
    "}).sort_values('p_value')\n",
    "print(likert_results[likert_results.p_value < 0.05].head(8))\n",
    "\n",
    "print(\"\\nFAST CHOICE MODEL:\")\n",
    "fast_results = pd.DataFrame({\n",
    "    'coefficient': model_fast.params,\n",
    "    'p_value': model_fast.pvalues\n",
    "}).sort_values('p_value')\n",
    "print(fast_results[fast_results.p_value < 0.05].head(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2978426",
   "metadata": {},
   "source": [
    "The above models are good, but before we interpret, aren't we just predicting preparedness with preparedness through the use of 'I'm ready for my next step.' Clearly, this is driving a substantial amount of variation.\n",
    "\n",
    "Let's try removing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eef427b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LIKERT MODEL SUMMARY (without el_lgen) ===\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:             prep_level   R-squared:                       0.490\n",
      "Model:                            OLS   Adj. R-squared:                  0.486\n",
      "Method:                 Least Squares   F-statistic:                     121.7\n",
      "Date:                Tue, 05 Aug 2025   Prob (F-statistic):               0.00\n",
      "Time:                        10:19:11   Log-Likelihood:                -6398.3\n",
      "No. Observations:                4210   AIC:                         1.286e+04\n",
      "Df Residuals:                    4176   BIC:                         1.308e+04\n",
      "Df Model:                          33                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===============================================================================================\n",
      "                                  coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Intercept                       2.1837      0.174     12.567      0.000       1.843       2.524\n",
      "age[T.25-40]                    0.0947      0.043      2.217      0.027       0.011       0.178\n",
      "age[T.41-64]                    0.0510      0.045      1.143      0.253      -0.036       0.138\n",
      "country_sector[T.FR Tech]       0.2809      0.068      4.117      0.000       0.147       0.415\n",
      "country_sector[T.IT Fin]        0.3611      0.066      5.473      0.000       0.232       0.490\n",
      "country_sector[T.SP Fin]        0.5186      0.067      7.715      0.000       0.387       0.650\n",
      "country_sector[T.UK Energy]     0.2898      0.065      4.452      0.000       0.162       0.417\n",
      "country_sector[T.US Pharma]     0.3548      0.067      5.288      0.000       0.223       0.486\n",
      "country_sector[T.US Tech]       0.2828      0.066      4.310      0.000       0.154       0.412\n",
      "el_lbal_anx                    -0.0449      0.008     -5.369      0.000      -0.061      -0.028\n",
      "el_lbal_sat                    -0.0276      0.012     -2.374      0.018      -0.050      -0.005\n",
      "el_lcnt_com                    -0.0034      0.012     -0.289      0.772      -0.026       0.019\n",
      "el_lcnt_psy                     0.0373      0.012      3.068      0.002       0.013       0.061\n",
      "el_lcon_inc                     0.0093      0.011      0.809      0.418      -0.013       0.032\n",
      "el_lcon_soc                     0.0003      0.009      0.030      0.976      -0.017       0.017\n",
      "el_lcont_con                    0.0540      0.012      4.352      0.000       0.030       0.078\n",
      "el_lcont_imp                    0.0675      0.012      5.418      0.000       0.043       0.092\n",
      "el_ljou_opt                     0.0160      0.010      1.567      0.117      -0.004       0.036\n",
      "el_ljou_pro                     0.1064      0.011      9.960      0.000       0.085       0.127\n",
      "el_lres_fin                     0.0363      0.009      4.050      0.000       0.019       0.054\n",
      "el_lres_ski                     0.1513      0.013     11.531      0.000       0.126       0.177\n",
      "el_ltra_aut                     0.0836      0.013      6.687      0.000       0.059       0.108\n",
      "el_ltra_des                     0.1243      0.009     13.852      0.000       0.107       0.142\n",
      "wfl_lcarprom_d                  0.0053      0.020      0.261      0.794      -0.034       0.045\n",
      "wfl_lcollea                    -0.0543      0.018     -3.050      0.002      -0.089      -0.019\n",
      "wfl_lenjhyb                     0.0629      0.016      3.926      0.000       0.031       0.094\n",
      "wfl_lexcess_wk                  0.0212      0.014      1.492      0.136      -0.007       0.049\n",
      "wfl_lfin_lair                   0.0315      0.015      2.072      0.038       0.002       0.061\n",
      "wfl_llearn_dev                 -0.0138      0.021     -0.661      0.508      -0.055       0.027\n",
      "wfl_lmean_full                  0.0036      0.020      0.177      0.859      -0.036       0.044\n",
      "wfl_lpoorman                    0.0248      0.016      1.535      0.125      -0.007       0.056\n",
      "wfl_ltoxic                     -0.0070      0.016     -0.448      0.654      -0.038       0.024\n",
      "wfl_luse_skills                 0.0543      0.022      2.480      0.013       0.011       0.097\n",
      "wfl_lwellcomp                   0.0254      0.017      1.462      0.144      -0.009       0.059\n",
      "==============================================================================\n",
      "Omnibus:                      449.615   Durbin-Watson:                   1.995\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              793.124\n",
      "Skew:                          -0.728   Prob(JB):                    5.96e-173\n",
      "Kurtosis:                       4.550   Cond. No.                         277.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "=== FAST CHOICE MODEL SUMMARY (without es_fgen) ===\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:             prep_level   R-squared:                       0.320\n",
      "Model:                            OLS   Adj. R-squared:                  0.315\n",
      "Method:                 Least Squares   F-statistic:                     59.46\n",
      "Date:                Tue, 05 Aug 2025   Prob (F-statistic):          1.04e-318\n",
      "Time:                        10:19:11   Log-Likelihood:                -6980.1\n",
      "No. Observations:                4195   AIC:                         1.403e+04\n",
      "Df Residuals:                    4161   BIC:                         1.424e+04\n",
      "Df Model:                          33                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===============================================================================================\n",
      "                                  coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Intercept                       5.6992      0.128     44.529      0.000       5.448       5.950\n",
      "age[T.25-40]                    0.1789      0.049      3.632      0.000       0.082       0.275\n",
      "age[T.41-64]                    0.1084      0.051      2.132      0.033       0.009       0.208\n",
      "country_sector[T.FR Tech]       0.1990      0.076      2.613      0.009       0.050       0.348\n",
      "country_sector[T.IT Fin]        0.2476      0.077      3.215      0.001       0.097       0.399\n",
      "country_sector[T.SP Fin]        0.2542      0.078      3.275      0.001       0.102       0.406\n",
      "country_sector[T.UK Energy]    -0.0312      0.076     -0.409      0.682      -0.180       0.118\n",
      "country_sector[T.US Pharma]     0.3340      0.078      4.305      0.000       0.182       0.486\n",
      "country_sector[T.US Tech]       0.1465      0.076      1.918      0.055      -0.003       0.296\n",
      "es_fbal_anx                    -0.0759      0.056     -1.364      0.173      -0.185       0.033\n",
      "es_fbal_sat                     0.0366      0.054      0.678      0.498      -0.069       0.142\n",
      "es_fcnt_com                     0.0258      0.057      0.455      0.649      -0.085       0.137\n",
      "es_fcnt_psy                     0.2027      0.060      3.354      0.001       0.084       0.321\n",
      "es_fcon_inc                     0.0474      0.054      0.885      0.376      -0.058       0.152\n",
      "es_fcon_soc                    -0.0674      0.059     -1.145      0.252      -0.183       0.048\n",
      "es_fcont_con                    0.3734      0.061      6.140      0.000       0.254       0.493\n",
      "es_fcont_imp                    0.4259      0.057      7.436      0.000       0.314       0.538\n",
      "es_fjou_opt                     0.3280      0.054      6.029      0.000       0.221       0.435\n",
      "es_fjou_pro                     0.4820      0.058      8.365      0.000       0.369       0.595\n",
      "es_fres_fin                     0.2959      0.057      5.192      0.000       0.184       0.408\n",
      "es_fres_ski                     0.2697      0.062      4.374      0.000       0.149       0.391\n",
      "es_ftra_aut                     0.3816      0.061      6.300      0.000       0.263       0.500\n",
      "es_ftra_des                     0.6241      0.056     11.171      0.000       0.515       0.734\n",
      "wfs_fcarprom                    0.1149      0.084      1.368      0.171      -0.050       0.280\n",
      "wfs_fcollea                     0.0357      0.084      0.425      0.671      -0.129       0.200\n",
      "wfs_fenjhyb                     0.2451      0.082      2.988      0.003       0.084       0.406\n",
      "wfs_fexcess_wk                  0.1055      0.070      1.515      0.130      -0.031       0.242\n",
      "wfs_ffin_fair                   0.0529      0.088      0.602      0.548      -0.119       0.225\n",
      "wfs_flearn_dev                  0.0346      0.087      0.399      0.690      -0.136       0.205\n",
      "wfs_fmean_full                  0.0622      0.084      0.739      0.460      -0.103       0.227\n",
      "wfs_fpoorman                   -0.0028      0.075     -0.037      0.970      -0.149       0.144\n",
      "wfs_ftoxic                      0.0110      0.077      0.143      0.886      -0.139       0.161\n",
      "wfs_fuse_skills                 0.1061      0.090      1.180      0.238      -0.070       0.282\n",
      "wfs_fwellcomp                   0.0993      0.076      1.305      0.192      -0.050       0.248\n",
      "==============================================================================\n",
      "Omnibus:                      345.929   Durbin-Watson:                   2.008\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              478.737\n",
      "Skew:                          -0.682   Prob(JB):                    1.11e-104\n",
      "Kurtosis:                       3.937   Cond. No.                         21.1\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "=== MODEL COMPARISON (without gen variables) ===\n",
      "Likert Model - R²: 0.490, Adj R²: 0.486, N: 4210.0\n",
      "Fast Choice Model - R²: 0.320, Adj R²: 0.315, N: 4195.0\n",
      "\n",
      "=== TOP SIGNIFICANT PREDICTORS (p < 0.05) ===\n",
      "LIKERT MODEL:\n",
      "                             coefficient       p_value\n",
      "el_ltra_des                     0.124262  1.080811e-42\n",
      "Intercept                       2.183655  1.401061e-35\n",
      "el_lres_ski                     0.151307  2.647335e-30\n",
      "el_ljou_pro                     0.106363  4.104474e-23\n",
      "country_sector[T.SP Fin]        0.518554  1.502683e-14\n",
      "el_ltra_aut                     0.083620  2.581040e-11\n",
      "country_sector[T.IT Fin]        0.361091  4.695903e-08\n",
      "el_lcont_imp                    0.067471  6.377647e-08\n",
      "el_lbal_anx                    -0.044861  8.345535e-08\n",
      "country_sector[T.US Pharma]     0.354794  1.301615e-07\n",
      "\n",
      "FAST CHOICE MODEL:\n",
      "                             coefficient       p_value\n",
      "Intercept                       5.699161  0.000000e+00\n",
      "es_ftra_des                     0.624093  1.436350e-28\n",
      "es_fjou_pro                     0.482024  8.092290e-17\n",
      "es_fcont_imp                    0.425918  1.255802e-13\n",
      "es_ftra_aut                     0.381551  3.285766e-10\n",
      "es_fcont_con                    0.373352  9.047941e-10\n",
      "es_fjou_opt                     0.327999  1.794444e-09\n",
      "es_fres_fin                     0.295941  2.180677e-07\n",
      "es_fres_ski                     0.269662  1.247789e-05\n",
      "country_sector[T.US Pharma]     0.333965  1.705292e-05\n"
     ]
    }
   ],
   "source": [
    "# Exclude el_lgen and es_fgen from models\n",
    "likert_data = data[['prep_level'] + \n",
    "                   [col for col in data.columns if col.startswith('el_') and col != 'el_lgen'] +\n",
    "                   [col for col in data.columns if col.startswith('wfl_')] +\n",
    "                   ['age', 'country_sector']].copy()\n",
    "\n",
    "fast_data = data[['prep_level'] + \n",
    "                 [col for col in data.columns if col.startswith('es_') and col != 'es_fgen'] +\n",
    "                 [col for col in data.columns if col.startswith('wfs_')] +\n",
    "                 ['age', 'country_sector']].copy()\n",
    "\n",
    "# Apply transformations\n",
    "# Transform emotional: -100 to +100 with 9 points → 1 to 9\n",
    "el_cols = [col for col in likert_data.columns if col.startswith('el_')]\n",
    "for col in el_cols:\n",
    "    likert_data[col] = (likert_data[col] + 100) / 25 + 1\n",
    "\n",
    "# Transform workplace: 0 to 100 with 5 points → 1 to 5\n",
    "wfl_cols = [col for col in likert_data.columns if col.startswith('wfl_')]\n",
    "for col in wfl_cols:\n",
    "    likert_data[col] = likert_data[col] / 25 + 1\n",
    "\n",
    "# Transform emotional: -100 to +100 → -1 to +1\n",
    "es_cols = [col for col in fast_data.columns if col.startswith('es_')]\n",
    "for col in es_cols:\n",
    "    fast_data[col] = fast_data[col] / 100\n",
    "\n",
    "# Transform workplace: 0 to 100 → 0 to 1\n",
    "wfs_cols = [col for col in fast_data.columns if col.startswith('wfs_')]\n",
    "for col in wfs_cols:\n",
    "    fast_data[col] = fast_data[col] / 100\n",
    "\n",
    "# Drop missing values\n",
    "likert_clean = likert_data.dropna()\n",
    "fast_clean = fast_data.dropna()\n",
    "\n",
    "# Fit models without el_lgen and es_fgen\n",
    "model_likert = ols('prep_level ~ ' + ' + '.join([col for col in likert_clean.columns if col != 'prep_level']), \n",
    "                   data=likert_clean).fit()\n",
    "\n",
    "model_fast = ols('prep_level ~ ' + ' + '.join([col for col in fast_clean.columns if col != 'prep_level']), \n",
    "                 data=fast_clean).fit()\n",
    "\n",
    "# RESULTS AFTER EXCLUDING el_lgen and es_fgen\n",
    "print(\"=== LIKERT MODEL SUMMARY (without el_lgen) ===\")\n",
    "print(model_likert.summary())\n",
    "\n",
    "print(\"\\n=== FAST CHOICE MODEL SUMMARY (without es_fgen) ===\")\n",
    "print(model_fast.summary())\n",
    "\n",
    "# MODEL COMPARISON\n",
    "print(\"\\n=== MODEL COMPARISON (without gen variables) ===\")\n",
    "print(f\"Likert Model - R²: {model_likert.rsquared:.3f}, Adj R²: {model_likert.rsquared_adj:.3f}, N: {model_likert.nobs}\")\n",
    "print(f\"Fast Choice Model - R²: {model_fast.rsquared:.3f}, Adj R²: {model_fast.rsquared_adj:.3f}, N: {model_fast.nobs}\")\n",
    "\n",
    "# TOP SIGNIFICANT PREDICTORS\n",
    "print(\"\\n=== TOP SIGNIFICANT PREDICTORS (p < 0.05) ===\")\n",
    "print(\"LIKERT MODEL:\")\n",
    "likert_results = pd.DataFrame({\n",
    "    'coefficient': model_likert.params,\n",
    "    'p_value': model_likert.pvalues\n",
    "}).sort_values('p_value')\n",
    "print(likert_results[likert_results.p_value < 0.05].head(10))\n",
    "\n",
    "print(\"\\nFAST CHOICE MODEL:\")\n",
    "fast_results = pd.DataFrame({\n",
    "    'coefficient': model_fast.params,\n",
    "    'p_value': model_fast.pvalues\n",
    "}).sort_values('p_value')\n",
    "print(fast_results[fast_results.p_value < 0.05].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84181c61",
   "metadata": {},
   "source": [
    "#### Results: True Drivers of Preparedness (Excluding Circular Predictors)\n",
    "\n",
    "#### Key Summary States\n",
    "- **Likert Model**: R² dropped from 0.74 → 0.49 (but now meaningful)\n",
    "- **Fast Choice Model**: R² dropped from 0.53 → 0.32 (honest variance with a harder prediction task)\n",
    "- **Coefficients are now much larger and interpretable**\n",
    "\n",
    "#### Top Emotional Drivers of Preparedness\n",
    "\n",
    "#### Likert Model - Clear Predictors:\n",
    "1. **`el_lres_ski` (0.151)**: \"I've got the skills to progress\"\n",
    "  - **Skills confidence is the #1 emotional driver**\n",
    "  - Moving 1 point on 9-point scale = +0.15 prep points\n",
    "\n",
    "2. **`el_ltra_des` (0.124)**: \"I want a new start\" \n",
    "  - **Transformation desire** - strong predictor\n",
    "  - People wanting change are more prepared\n",
    "\n",
    "3. **`el_ljou_pro` (0.106)**: \"I've set myself goals\"\n",
    "  - **Goal-setting behavior** drives preparedness\n",
    "\n",
    "4. **`el_ltra_aut` (0.084)**: \"I've got freedom to change\"\n",
    "  - **Autonomy/agency** in transformation\n",
    "\n",
    "5. **`el_lcont_imp` (0.067)**: \"I believe in myself\"\n",
    "  - **Self-efficacy** matters\n",
    "\n",
    "### Fast Choice Model - Confirms Pattern:\n",
    "1. **`es_ftra_des` (0.624)**: \"I want a new start\" - **Transformation desire**\n",
    "2. **`es_fjou_pro` (0.482)**: \"I've set myself goals\" - **Goal orientation**  \n",
    "3. **`es_fcont_imp` (0.426)**: \"I believe in myself\" - **Self-efficacy**\n",
    "4. **`es_ftra_aut` (0.382)**: \"I've got freedom to change\" - **Autonomy**\n",
    "5. **`es_fcont_con` (0.373)**: \"I control my next step\" - **Personal control**\n",
    "\n",
    "---\n",
    "\n",
    "#### Top Workplace Drivers of Preparedness\n",
    "\n",
    "#### Likert Model:\n",
    "1. **`wfl_lenjhyb` (0.063)**: \"I enjoy hybrid working\"\n",
    "  - **Work flexibility satisfaction**\n",
    "\n",
    "2. **`wfl_luse_skills` (0.054)**: \"I can use my skills\"\n",
    "  - **Skills utilization** at work\n",
    "\n",
    "3. **`wfl_lfin_lair` (0.032)**: \"My salary is unfair compared to colleagues\"\n",
    "  - **Pay equity concerns** drive preparation to leave\n",
    "\n",
    "4. **`wfl_lcollea` (-0.054)**: \"I don't get along with my colleagues\"\n",
    "  - **Relationship problems** increase preparedness (negative relationship)\n",
    "\n",
    "#### Fast Choice Model:\n",
    "1. **`wfs_fenjhyb` (0.245)**: \"I enjoy hybrid working\" - **Work flexibility**\n",
    "2. Minor workplace effects compared to emotional factors\n",
    "\n",
    "---\n",
    "\n",
    "**Core Emotional Profile:**\n",
    "- **High skills confidence** (\"I've got the skills\")\n",
    "- **Strong transformation desire** (\"I want a new start\") \n",
    "- **Goal-oriented behavior** (\"I've set goals\")\n",
    "- **Sense of personal control** (\"I control my next step\")\n",
    "- **Self-efficacy** (\"I believe in myself\")\n",
    "\n",
    "**Workplace Triggers:**\n",
    "- **Work arrangement dissatisfaction** (hybrid working preferences)\n",
    "- **Skills underutilization** \n",
    "- **Pay inequity perceptions**\n",
    "- **Poor colleague relationships**\n",
    "\n",
    "**Key Insight**: Preparedness is primarily **internally driven** (emotional factors 2-3x stronger than workplace factors). People prepare when they feel confident, want change, and believe they can control their path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064cd858",
   "metadata": {},
   "source": [
    "### Possible extensions to the model:\n",
    "\n",
    "- Add Elastic Net specification to deal with our multicollinearity (a VIF of 6 is reasonably high but not too concerning)\n",
    "\n",
    "- Integrate the latent factors from the other notebook as predictors of preparedness, would also address the multicollinearity issue\n",
    "\n",
    "- Remove Anxiety (el_lbal_anx) which might be masking other predictors\n",
    "\n",
    "- Find a similar methodological study to see how regression was used with 25+ survey variables\n",
    "\n",
    "- Add theoretically informed emotional interaction terms like:\n",
    "'el_lres_ski * el_ltra_des'  # Skills confidence × Change desire\n",
    "'el_ljou_pro * el_lres_ski'  # Goal-setting × Skills confidence\n",
    "\n",
    "- Add a simplified global model with just the top 5-7 predictors only to see (but may have multicollinearity concerns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6644d67a",
   "metadata": {},
   "source": [
    "# Part 2: Comparing Implicit vs. Explicit Predictors\n",
    "\n",
    "Do implicit responses add incremental predictive value over explicit ones?\n",
    "\n",
    "We already saw that explicit predictors explain more of the total variance in preparedness than implicit predictors (as explained by the R-squared). Let's investigate if this trend persists, as well as dive deeper into what the differences are in these predictors.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b31ab12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PART 2A: IMPLICIT VS EXPLICIT PREDICTORS COMPARISON\n",
      "================================================================================\n",
      "\n",
      "1. EXPLICIT-ONLY MODEL\n",
      "----------------------------------------\n",
      "R²: 0.490\n",
      "Adj R²: 0.486\n",
      "N: 4210.0\n",
      "\n",
      "2. IMPLICIT-ONLY MODEL\n",
      "----------------------------------------\n",
      "R²: 0.320\n",
      "Adj R²: 0.315\n",
      "N: 4195.0\n"
     ]
    }
   ],
   "source": [
    "# Part 2: Comparing Implicit vs. Explicit Predictors\n",
    "# Test incremental predictive value of implicit responses over explicit ones\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ============================================================================\n",
    "# PART 2A: COMPARE IMPLICIT VS EXPLICIT PREDICTORS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PART 2A: IMPLICIT VS EXPLICIT PREDICTORS COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Exclude circular predictors (el_lgen, es_fgen)\n",
    "explicit_emotional = [col for col in data.columns if col.startswith('el_') and col != 'el_lgen']\n",
    "explicit_workplace = [col for col in data.columns if col.startswith('wfl_')]\n",
    "implicit_emotional = [col for col in data.columns if col.startswith('es_') and col != 'es_fgen'] \n",
    "implicit_workplace = [col for col in data.columns if col.startswith('wfs_')]\n",
    "\n",
    "# Transform variables to comparable scales\n",
    "def prepare_explicit_data(df):\n",
    "    \"\"\"Transform explicit (Likert) variables to 1-9 and 1-5 scales\"\"\"\n",
    "    df_prep = df.copy()\n",
    "    \n",
    "    # Emotional: -100 to +100 → 1 to 9 \n",
    "    for col in explicit_emotional:\n",
    "        if col in df_prep.columns:\n",
    "            df_prep[col] = (df_prep[col] + 100) / 25 + 1\n",
    "    \n",
    "    # Workplace: 0 to 100 → 1 to 5\n",
    "    for col in explicit_workplace:\n",
    "        if col in df_prep.columns:\n",
    "            df_prep[col] = df_prep[col] / 25 + 1\n",
    "    \n",
    "    return df_prep\n",
    "\n",
    "def prepare_implicit_data(df):\n",
    "    \"\"\"Transform implicit (fast choice) variables to -1 to +1 and 0 to 1 scales\"\"\"\n",
    "    df_prep = df.copy()\n",
    "    \n",
    "    # Emotional: scale to -1 to +1\n",
    "    for col in implicit_emotional:\n",
    "        if col in df_prep.columns:\n",
    "            df_prep[col] = df_prep[col] / 100\n",
    "    \n",
    "    # Workplace: scale to 0 to 1  \n",
    "    for col in implicit_workplace:\n",
    "        if col in df_prep.columns:\n",
    "            df_prep[col] = df_prep[col] / 100\n",
    "    \n",
    "    return df_prep\n",
    "\n",
    "# Create datasets\n",
    "explicit_vars = explicit_emotional + explicit_workplace + ['age', 'country_sector', 'prep_level']\n",
    "implicit_vars = implicit_emotional + implicit_workplace + ['age', 'country_sector', 'prep_level']\n",
    "\n",
    "explicit_data = prepare_explicit_data(data[explicit_vars]).dropna()\n",
    "implicit_data = prepare_implicit_data(data[implicit_vars]).dropna()\n",
    "\n",
    "# Build models\n",
    "print(\"\\n1. EXPLICIT-ONLY MODEL\")\n",
    "print(\"-\" * 40)\n",
    "explicit_predictors = [col for col in explicit_data.columns if col != 'prep_level']\n",
    "explicit_formula = 'prep_level ~ ' + ' + '.join(explicit_predictors)\n",
    "explicit_model = ols(explicit_formula, data=explicit_data).fit()\n",
    "\n",
    "print(f\"R²: {explicit_model.rsquared:.3f}\")\n",
    "print(f\"Adj R²: {explicit_model.rsquared_adj:.3f}\")\n",
    "print(f\"N: {explicit_model.nobs}\")\n",
    "\n",
    "print(\"\\n2. IMPLICIT-ONLY MODEL\") \n",
    "print(\"-\" * 40)\n",
    "implicit_predictors = [col for col in implicit_data.columns if col != 'prep_level']\n",
    "implicit_formula = 'prep_level ~ ' + ' + '.join(implicit_predictors)\n",
    "implicit_model = ols(implicit_formula, data=implicit_data).fit()\n",
    "\n",
    "print(f\"R²: {implicit_model.rsquared:.3f}\")\n",
    "print(f\"Adj R²: {implicit_model.rsquared_adj:.3f}\")\n",
    "print(f\"N: {implicit_model.nobs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200f2b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 2B: INCREMENTAL PREDICTIVE VALUE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Matched sample size: 4194\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 2B: INCREMENTAL PREDICTIVE VALUE TEST\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 2B: INCREMENTAL PREDICTIVE VALUE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create matched dataset (same observations for fair comparison)\n",
    "# Find intersection of variables across both datasets\n",
    "common_vars = ['prep_level', 'age', 'country_sector']\n",
    "all_explicit = [col for col in explicit_emotional + explicit_workplace \n",
    "                if col in explicit_data.columns]\n",
    "all_implicit = [col for col in implicit_emotional + implicit_workplace \n",
    "               if col in implicit_data.columns]\n",
    "\n",
    "# Create combined dataset\n",
    "combined_vars = common_vars + all_explicit + all_implicit\n",
    "combined_data = data[combined_vars].copy()\n",
    "\n",
    "# Apply transformations\n",
    "combined_data = prepare_explicit_data(combined_data)\n",
    "combined_data = prepare_implicit_data(combined_data)\n",
    "\n",
    "# Drop missing values for complete case analysis\n",
    "combined_clean = combined_data.dropna()\n",
    "print(f\"\\nMatched sample size: {len(combined_clean)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bfed421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. HIERARCHICAL REGRESSION ANALYSIS\n",
      "--------------------------------------------------\n",
      "Model 1 (Demographics): R² = 0.048\n",
      "Model 2 (Demo + Explicit): R² = 0.490\n",
      "ΔR² from Demographics: 0.442\n",
      "Model 3 (Demo + Explicit + Implicit): R² = 0.511\n",
      "ΔR² from Explicit Model: 0.020\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================ \n",
    "# HIERARCHICAL REGRESSION: Test incremental value\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n3. HIERARCHICAL REGRESSION ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Model 1: Demographics only (baseline)\n",
    "demo_vars = ['age', 'country_sector']\n",
    "demo_formula = 'prep_level ~ ' + ' + '.join(demo_vars)\n",
    "model_demo = ols(demo_formula, data=combined_clean).fit()\n",
    "\n",
    "print(f\"Model 1 (Demographics): R² = {model_demo.rsquared:.3f}\")\n",
    "\n",
    "# Model 2: Demographics + Explicit predictors  \n",
    "explicit_vars_clean = [col for col in all_explicit if col in combined_clean.columns]\n",
    "model2_vars = demo_vars + explicit_vars_clean\n",
    "model2_formula = 'prep_level ~ ' + ' + '.join(model2_vars)\n",
    "model_explicit = ols(model2_formula, data=combined_clean).fit()\n",
    "\n",
    "print(f\"Model 2 (Demo + Explicit): R² = {model_explicit.rsquared:.3f}\")\n",
    "print(f\"ΔR² from Demographics: {model_explicit.rsquared - model_demo.rsquared:.3f}\")\n",
    "\n",
    "# Model 3: Demographics + Explicit + Implicit predictors\n",
    "implicit_vars_clean = [col for col in all_implicit if col in combined_clean.columns]\n",
    "model3_vars = demo_vars + explicit_vars_clean + implicit_vars_clean\n",
    "model3_formula = 'prep_level ~ ' + ' + '.join(model3_vars)\n",
    "model_combined = ols(model3_formula, data=combined_clean).fit()\n",
    "\n",
    "print(f\"Model 3 (Demo + Explicit + Implicit): R² = {model_combined.rsquared:.3f}\")\n",
    "print(f\"ΔR² from Explicit Model: {model_combined.rsquared - model_explicit.rsquared:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed850f51",
   "metadata": {},
   "source": [
    "This is key: adding the implicit variables does slightly increase predictive value from the explicit ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8631e252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. REVERSE ORDER TEST\n",
      "------------------------------\n",
      "Model 2B (Demo + Implicit): R² = 0.320\n",
      "ΔR² from Demographics: 0.272\n",
      "Model 3B (Demo + Implicit + Explicit): R² = 0.511\n",
      "ΔR² from Implicit Model: 0.190\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ALTERNATIVE ORDER: Test if implicit adds more when entered first\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n4. REVERSE ORDER TEST\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Model 2B: Demographics + Implicit predictors first\n",
    "model2b_vars = demo_vars + implicit_vars_clean  \n",
    "model2b_formula = 'prep_level ~ ' + ' + '.join(model2b_vars)\n",
    "model_implicit = ols(model2b_formula, data=combined_clean).fit()\n",
    "\n",
    "print(f\"Model 2B (Demo + Implicit): R² = {model_implicit.rsquared:.3f}\")\n",
    "print(f\"ΔR² from Demographics: {model_implicit.rsquared - model_demo.rsquared:.3f}\")\n",
    "\n",
    "# Model 3B: Demographics + Implicit + Explicit (same as Model 3)\n",
    "print(f\"Model 3B (Demo + Implicit + Explicit): R² = {model_combined.rsquared:.3f}\")\n",
    "print(f\"ΔR² from Implicit Model: {model_combined.rsquared - model_implicit.rsquared:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c1afc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. STATISTICAL SIGNIFICANCE OF INCREMENTAL VALUE\n",
      "-------------------------------------------------------\n",
      "F-test: Implicit over Explicit\n",
      "F(25, 4135.0) = 6.832\n",
      "Critical F (α=0.05): 1.509\n",
      "p-value: 0.0000\n",
      "Significant: Yes\n",
      "\n",
      "F-test: Explicit over Implicit\n",
      "F(25, 4135.0) = 64.292\n",
      "Critical F (α=0.05): 1.509\n",
      "p-value: 0.0000\n",
      "Significant: Yes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# STATISTICAL SIGNIFICANCE TESTS  \n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n5. STATISTICAL SIGNIFICANCE OF INCREMENTAL VALUE\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "# F-test for incremental R²\n",
    "def f_test_incremental(reduced_model, full_model):\n",
    "    \"\"\"Calculate F-test for incremental R² between nested models\"\"\"\n",
    "    r2_reduced = reduced_model.rsquared\n",
    "    r2_full = full_model.rsquared\n",
    "    \n",
    "    n = full_model.nobs\n",
    "    k_reduced = len(reduced_model.params) - 1  # exclude intercept\n",
    "    k_full = len(full_model.params) - 1\n",
    "    \n",
    "    delta_r2 = r2_full - r2_reduced\n",
    "    delta_k = k_full - k_reduced\n",
    "    \n",
    "    f_stat = (delta_r2 / delta_k) / ((1 - r2_full) / (n - k_full - 1))\n",
    "    \n",
    "    return f_stat, delta_k, n - k_full - 1\n",
    "\n",
    "# Test if implicit adds significant value over explicit\n",
    "f_stat, df1, df2 = f_test_incremental(model_explicit, model_combined)\n",
    "print(f\"F-test: Implicit over Explicit\")\n",
    "print(f\"F({df1}, {df2}) = {f_stat:.3f}\")\n",
    "\n",
    "# Critical F-value at α = 0.05\n",
    "from scipy.stats import f\n",
    "critical_f = f.ppf(0.95, df1, df2)\n",
    "p_value = 1 - f.cdf(f_stat, df1, df2)\n",
    "print(f\"Critical F (α=0.05): {critical_f:.3f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "print(f\"Significant: {'Yes' if f_stat > critical_f else 'No'}\")\n",
    "\n",
    "# Test if explicit adds significant value over implicit  \n",
    "f_stat2, df1_2, df2_2 = f_test_incremental(model_implicit, model_combined)\n",
    "print(f\"\\nF-test: Explicit over Implicit\")\n",
    "print(f\"F({df1_2}, {df2_2}) = {f_stat2:.3f}\")\n",
    "critical_f2 = f.ppf(0.95, df1_2, df2_2)\n",
    "p_value2 = 1 - f.cdf(f_stat2, df1_2, df2_2)\n",
    "print(f\"Critical F (α=0.05): {critical_f2:.3f}\")\n",
    "print(f\"p-value: {p_value2:.4f}\")\n",
    "print(f\"Significant: {'Yes' if f_stat2 > critical_f2 else 'No'}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5582bd79",
   "metadata": {},
   "source": [
    "### F-Test for Incremental Predictive Value\n",
    "\n",
    "The F-test determines whether adding implicit measures provides **statistically significant** improvement over explicit measures alone.\n",
    "\n",
    "**What it tests:**\n",
    "- H₀: Implicit measures add no real predictive value (improvement = random noise)\n",
    "- H₁: Implicit measures genuinely enhance prediction\n",
    "\n",
    "**The calculation:**\n",
    "F = (ΔR² / additional_predictors) / (unexplained_variance / remaining_df)\n",
    "\n",
    "**Interpretation:**\n",
    "- **p < 0.05**: Improvement is statistically significant - implicit measures capture unique signal\n",
    "- **p > 0.05**: Improvement could be due to chance - may just be overfitting\n",
    "\n",
    "**Why this matters:**\n",
    "Without this test, a small R² increase (e.g., 0.49 → 0.52) might seem meaningful when it's actually statistical noise. The F-test distinguishes between:\n",
    "- Real predictive gain (implicit measures are valuable)\n",
    "- Artifact of adding more variables (mechanically increases R²)\n",
    "\n",
    "This tells us whether the complexity of collecting both explicit and implicit data is statistically justified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ab82d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. CROSS-VALIDATION ANALYSIS\n",
      "-----------------------------------\n",
      "Cross-validated R² (mean ± std):\n",
      "Demographics: 0.044 ± 0.010\n",
      "Explicit: 0.480 ± 0.027\n",
      "Implicit: 0.308 ± 0.033\n",
      "Combined: 0.494 ± 0.029\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CROSS-VALIDATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n6. CROSS-VALIDATION ANALYSIS\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Prepare data for sklearn\n",
    "X_demo = combined_clean[demo_vars]\n",
    "X_explicit = combined_clean[demo_vars + explicit_vars_clean]\n",
    "X_implicit = combined_clean[demo_vars + implicit_vars_clean] \n",
    "X_combined = combined_clean[demo_vars + explicit_vars_clean + implicit_vars_clean]\n",
    "y = combined_clean['prep_level']\n",
    "\n",
    "# Convert categorical variables to dummy variables\n",
    "X_demo_encoded = pd.get_dummies(X_demo, drop_first=True)\n",
    "X_explicit_encoded = pd.get_dummies(X_explicit, drop_first=True)\n",
    "X_implicit_encoded = pd.get_dummies(X_implicit, drop_first=True)\n",
    "X_combined_encoded = pd.get_dummies(X_combined, drop_first=True)\n",
    "\n",
    "# 5-fold cross-validation\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_demo = cross_val_score(LinearRegression(), X_demo_encoded, y, cv=cv, scoring='r2')\n",
    "cv_explicit = cross_val_score(LinearRegression(), X_explicit_encoded, y, cv=cv, scoring='r2')\n",
    "cv_implicit = cross_val_score(LinearRegression(), X_implicit_encoded, y, cv=cv, scoring='r2')\n",
    "cv_combined = cross_val_score(LinearRegression(), X_combined_encoded, y, cv=cv, scoring='r2')\n",
    "\n",
    "print(f\"Cross-validated R² (mean ± std):\")\n",
    "print(f\"Demographics: {cv_demo.mean():.3f} ± {cv_demo.std():.3f}\")\n",
    "print(f\"Explicit: {cv_explicit.mean():.3f} ± {cv_explicit.std():.3f}\")\n",
    "print(f\"Implicit: {cv_implicit.mean():.3f} ± {cv_implicit.std():.3f}\")\n",
    "print(f\"Combined: {cv_combined.mean():.3f} ± {cv_combined.std():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28f87bd",
   "metadata": {},
   "source": [
    "### Cross-Validation Analysis - Another check on our R-squared trend\n",
    "\n",
    "Cross-validation tests whether our models **generalize** to new data or just memorize the training sample.\n",
    "\n",
    "**Method:**\n",
    "- Split data into 5 folds\n",
    "- Train on 4 folds, test on 1 fold\n",
    "- Repeat 5 times, average the R² scores\n",
    "\n",
    "**What it reveals:**\n",
    "- **High CV R²**: Model captures real patterns that generalize\n",
    "- **Low CV R²**: Model is overfitting to sample-specific noise\n",
    "- **CV R² << Training R²**: Strong overfitting signal\n",
    "\n",
    "**Key comparisons:**\n",
    "- Compare CV performance across explicit, implicit, and combined models\n",
    "- If combined model's CV R² > explicit model's CV R², implicit measures add robust value\n",
    "- If CV R² drops substantially from training R², too many predictors for sample size\n",
    "\n",
    "**Why this matters:**\n",
    "The F-test tells us if improvement is statistically significant, but CV tells us if it's **practically robust**. A model might pass the F-test but fail CV if it's overfitted.\n",
    "\n",
    "Combined model shows higher CV R² than individual models, confirming implicit measures add generalizable predictive value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33715496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. DOMAIN-SPECIFIC INCREMENTAL VALUE\n",
      "---------------------------------------------\n",
      "EMOTIONAL DOMAIN:\n",
      "Explicit R²: 0.484\n",
      "Implicit R²: 0.315\n",
      "Combined R²: 0.503\n",
      "Incremental value of implicit: 0.019\n",
      "\n",
      "WORKPLACE DOMAIN:\n",
      "Explicit R²: 0.237\n",
      "Implicit R²: 0.127\n",
      "Combined R²: 0.240\n",
      "Incremental value of implicit: 0.003\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# DOMAIN-SPECIFIC ANALYSIS: EMOTIONAL VS WORKPLACE PREDICTORS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n7. DOMAIN-SPECIFIC INCREMENTAL VALUE\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Test emotional vs workplace domains separately\n",
    "explicit_emotional_clean = [col for col in explicit_emotional if col in combined_clean.columns]\n",
    "explicit_workplace_clean = [col for col in explicit_workplace if col in combined_clean.columns]\n",
    "implicit_emotional_clean = [col for col in implicit_emotional if col in combined_clean.columns]\n",
    "implicit_workplace_clean = [col for col in implicit_workplace if col in combined_clean.columns]\n",
    "\n",
    "# Emotional domain models\n",
    "emo_explicit_vars = demo_vars + explicit_emotional_clean\n",
    "emo_implicit_vars = demo_vars + implicit_emotional_clean\n",
    "emo_combined_vars = demo_vars + explicit_emotional_clean + implicit_emotional_clean\n",
    "\n",
    "emo_explicit_model = ols('prep_level ~ ' + ' + '.join(emo_explicit_vars), data=combined_clean).fit()\n",
    "emo_implicit_model = ols('prep_level ~ ' + ' + '.join(emo_implicit_vars), data=combined_clean).fit()\n",
    "emo_combined_model = ols('prep_level ~ ' + ' + '.join(emo_combined_vars), data=combined_clean).fit()\n",
    "\n",
    "print(f\"EMOTIONAL DOMAIN:\")\n",
    "print(f\"Explicit R²: {emo_explicit_model.rsquared:.3f}\")\n",
    "print(f\"Implicit R²: {emo_implicit_model.rsquared:.3f}\")\n",
    "print(f\"Combined R²: {emo_combined_model.rsquared:.3f}\")\n",
    "print(f\"Incremental value of implicit: {emo_combined_model.rsquared - emo_explicit_model.rsquared:.3f}\")\n",
    "\n",
    "# Workplace domain models  \n",
    "work_explicit_vars = demo_vars + explicit_workplace_clean\n",
    "work_implicit_vars = demo_vars + implicit_workplace_clean\n",
    "work_combined_vars = demo_vars + explicit_workplace_clean + implicit_workplace_clean\n",
    "\n",
    "work_explicit_model = ols('prep_level ~ ' + ' + '.join(work_explicit_vars), data=combined_clean).fit()\n",
    "work_implicit_model = ols('prep_level ~ ' + ' + '.join(work_implicit_vars), data=combined_clean).fit()\n",
    "work_combined_model = ols('prep_level ~ ' + ' + '.join(work_combined_vars), data=combined_clean).fit()\n",
    "\n",
    "print(f\"\\nWORKPLACE DOMAIN:\")\n",
    "print(f\"Explicit R²: {work_explicit_model.rsquared:.3f}\")\n",
    "print(f\"Implicit R²: {work_implicit_model.rsquared:.3f}\")\n",
    "print(f\"Combined R²: {work_combined_model.rsquared:.3f}\")\n",
    "print(f\"Incremental value of implicit: {work_combined_model.rsquared - work_explicit_model.rsquared:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e6356a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Domain-Specific Analysis: Emotional vs Workplace Predictors\n",
    "\n",
    "Tests whether implicit measures add more value in **emotional** vs **workplace** domains. \n",
    "\n",
    "Key insight from results:\n",
    "\n",
    "Emotional domain: Implicit adds 1.9% incremental value (0.484 → 0.503)\n",
    "Workplace domain: Implicit adds only 0.3% incremental value (0.237 → 0.240)\n",
    "Emotional constructs benefit more from implicit measurement than workplace constructs\n",
    "\n",
    "Also, Emotional Domain is generally more predictive of preparedness than the workplace domain.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
